{
  "name": "The Claw",
  "tagline": "A fully autonomous Uber-bot for stuffed animal transportation.",
  "body": "## Table of Contents\r\n1. [Introduction](#1-introduction)\r\n2. [Mechanical Design](#2-mechanical-design)\r\n3. [Electrical Design](#3-electrical-design)\r\n    1. [Tapefinding](#31-tapefinding)\r\n    2. [Infrared sensing](#32-infrared-sensing)  \r\n4. [Software](#4-software)\r\n    1. [Navigation](#41-navigation)\r\n    2. [Intersection handling](#42-intersection-handling)\r\n    3. [Passenger pickup](#43-passenger-pickup)\r\n    4. [Tape following](#44-tape-following)\r\n    5. [Collision detection](#45-collision-detection)\r\n5. [Performance](#5-performance)\r\n\r\n## 1. Introduction\r\n![The robot, the myth, the legend](artview.jpg)\r\n\r\nThe Claw is a robot that can navigate a city whose roads are marked with black electrical tape on a white surface using 4 QRD 1114 reflectivity sensors, detect stuffed animal passengers with QSD 1124 infrared sensors, and pick them up with a large claw. The robot receives instructions from a TINAH board running mostly Arduino code. It was created and completed in the summer of second year for the ENPH 253: Introduction to Instrument Design course/competition at UBC by Tara Akhound-Sadegh, [Carol Ng](https://carolkng.github.io), Jamie O'Conniff, and Dylan Whitney.\r\n\r\n## 2. Mechanical design\r\nJamie and Dylan were responsible for the majority of the robot's mechanical design. All parts were designed in Solidworks and fabricated (in order of most used) with a combination of a waterjet cutter, machine shop/hand tools, 3D printers, and a laser cutter.\r\n\r\n![Initial claw design](claw.jpg)\r\n\r\nOur initial design concept featured storage for passengers holding up to 3 animals (with on in the claw), a fully articulated arm with 2 degrees of freedom on a rotating platform, and a bulk ejection system for quick dropoff of all passengers at the destination area. In order to maximize passenger storage and freedom of arm movement, the robot was to be around 14\" tall with a 11\" by 11\" base, not including the 0.5\" thick bumper on all sides to detect collisions with other robots. For the drivetrain, we opted to use two electric motors with one small gear reduction to keep the motor running at its most efficient RPM while keeping our robot moving at the desired target speed. The gears and wheels were both cut from clear polycarbonate on the waterjet cutter.\r\n\r\n![Final arm and claw](claw2.jpg)\r\nThe final robot features a similarly dimensioned arm which moves with one degree of freedom and three easily accessible vertical shelves for the TINAH board and all associated circuitry, which drastically reduces debugging and startup time. To reduce the likelihood of a collision with the sidewalk when performing U-turns, the shape of the chassis is much more rounded and compact, compared to the original. Since the original bumper design would only fit on straight edges, we opted to replace the protruding bumper with a skirt made of a thin continuous sheet of aluminum.\r\n\r\nAs there was not enough time to fully dimension every part in Solidworks before fabrication, our approach was to get files ready for the waterjet cutter or laser cutter as quickly as possible, then assemble parts using hand tools and non-permanent fasteners in order to see what works as quickly as possible. The major drawback of this approach was that the prototypes would not look nearly as aesthetically pleasing, and a part's holes or bends would not align as well as a completely CADed part would have.\r\n \r\n## 3. Electrical design\r\nTara and Carol were responsible for the construction, planning, and testing of all electronic components. Most circuit designs such as Scott's H-bridge can be found on the [Engineering Physics Project Lab](http://projectlab.engphys.ubc.ca) site, specifically on the ENPH 253 lab pages. \r\n\r\n### 3.1. Tapefinding\r\n![Tapefinding comparator circuit](QRD.jpg)\r\n\r\nFor tape following and intersection detection, we chose a system of 4 QRD 1114 sensors connected to a comparator circuit to set a threshold for HI/LO output depending on whether the robot sees tape. A testing system consisting of 4 LEDs was used to improve visual feedback when adjusting the thresholds of each QRD sensor and decrease setup time. Intersection detection was critical to our robot's ability to navigate the map and helped make sure that we didn't fall off the surface when we reached a sharp corner (more in [Software](#software)). Although we ended up soldering one potentiometer for each sensor, it turned out that the sensors were similar enough that the tuning could have been accomplished using one potentiometer.\r\n\r\n### 3.2 Infrared sensing\r\n![Infrared sensor circuit](QSD.jpg)\r\n\r\nTo detect passengers both at intersections and on the sidewalk, we soldered a total of 5 infrared sensor circuits: 3 for long range infrared sensing at intersections to determine which path was more likely to contain a passenger, and 2 on either side of the robot to see if a passenger is ready to be picked up. The circuits were made of 5 key components: a DC block, a 100x amplifier, a 1kHz bandpass filter, another amplifier with tunable amplification, and a peak detector.\r\n\r\n## 4 Software\r\nSoftware was jointly written by Tara and Carol using Arduino. Some PID control algorithms were reused from previous ENPH 253 labs, but other methods were written completely from scratch. You can download the Arduino project we used for the competition right [here](https://github.com/letsmakearobot/software/archive/master.zip). (The whole, single file source code will be released soon, I promise.)\r\n\r\n### 4.1 Navigation\r\nThe robot's copy of the city's map is very similar to the adjacency matrix of a bidirectional graph, with a few key differences:\r\n- The edge that the robot is travelling along is defined by two nodes:\r\n    - the node that the robot is travelling *from*, represented by the row index\r\n    - the node that the robot is travelling *to*, represented by the column index\r\n- All nonzero entries are replaced by a size 3 array representing the possible nodes the robot could travel to by turning left, right, or going straight ahead at the end of the edge (when an intersection is detected) \r\n\r\nBy hard coding a copy of the city's map as a lookup table in the robot's software, we were able to implement advanced intersection handling methods and minimize the time needed to return a stuffed animal passenger to the destination. Since the graph is fairly sparse (138/1200 entries are nonzero), we briefly considered using a compression of some sort to avoid reaching the memory limits on the TINAH, but decided against it in favor of decreasing the average computation time per loop.\r\n\r\n### 4.2 Intersection handling\r\nIntersection detection is handled by the two outer tape sensors. When the robot senses that it's at an intersection, it reads its front, long range IR sensors to find the direction (ie. left, right, or straight ahead) of the strongest IR signal, or the highest probability of having a passenger. Because we use the IR sensor data in conjunction with the stored location and map data, we avoid the possibility of camera flashes/autofocuses triggering false positives.\r\n\r\n### 4.3 Passenger pickup \r\n![Jack, I'm flying!](lift.gif)\r\n\r\nPassenger pickup was taken care of with modular methods for controlling the arm, in addition to experimentally determined positions for pickup and dropoff. \r\n\r\nDue to space constrictions, our robot was optimized for pickup on the left side of the sidewalk, so if a passenger was detected on the right, the robot would reverse itself and re-initiate the more advanced position algorithm before attempting to pick it up.\r\n\r\n### 4.4 Tape following\r\n![Oh, the early days.](spin.gif)\r\n\r\nTape following was accomplished using the PID control algorithms from a previous ENPH 253 lab, with our motors being powered at about 50% PWM at cruising speed. Based on the output of the two tapefinding sensors, the robot senses its position on the tape, and corrects its position by adjusting the speed of the two drive wheels independently.\r\n\r\n### 4.5 Collision detection\r\nFor our robot to qualify for the competition, we were required to install a bumper and collision detection system that would respond to a collision within 2 seconds. Using digital output pins on the TINAH, we were able to detect collisions on all sides except for the back of the robot, and turn on a dime immediately to avoid further altercations with the obstacle/rival robot. In order to maintain knowledge of our location on the map, we would reverse our previous/next nodes and continue to look for passengers or find an alternative route to the dropoff point.\r\n\r\n## 5 Performance\r\n![Short video of competition run!](comp.gif)\r\n\r\nIn a controlled testing circuit, the Claw was able to stop accurately for passengers almost 100% of the time, pick up the passengers some of the time (sometimes the actual claw would push them out), and reliably navigate the course without getting lost, even when handling collisions. However, on the competition surface, our robot suffered from IR blindness due to loose female headers that connected our IR circuit to our power distribution circuit, causing it to drive right by some passengers, and run in circles around one of the city blocks, since the floating inputs of the IR sensors were pulled up internally by the TINAH. All in all, the whole month-long ordeal was a great learning experience.",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}